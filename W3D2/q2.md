Q: 2
🔧 Problem 2: Build a Multimodal QA Agent
Create a simple multimodal QA web app that takes:

An image (via upload or URL)

A text question about the image

It should respond using a Vision + Language LLM (e.g., GPT-4o, Gemini, Claude 3).

Bonus:

Allow fallback to a text-only LLM if image analysis fails.
Visualize the bounding boxes or key areas if supported by the API response.
🧠 Key Concepts:
Multimodal models, GPT-4o, Gemini, Tool usage, API integration

📝 Submission Guidelines:
Push all code to a GitHub repo, including frontend and backend.
Provide screenshots or a Loom video in the README.md demonstrating usage.
Mention which LLM APIs were used and why.
Include a test report or sample outputs for 3 image-question pairs.
